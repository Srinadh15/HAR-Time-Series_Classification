{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861a6c14-87d9-4e6c-9338-ca017b4665a0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, recall_score, precision_score, confusion_matrix # For performance evaluation\n",
    "import numpy as np # For numerical operations\n",
    "import pandas as pd  # For data manipulation and analysis\n",
    "import random # For random operations\n",
    "\n",
    "# Import necessary components from scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder # For scaling features and encoding labels\n",
    "from sklearn.model_selection import StratifiedKFold # For stratified cross-validation\n",
    "from sklearn.utils.class_weight import compute_class_weight # For computing class weights to handle imbalances\n",
    "\n",
    "# Import necessary components from Keras\n",
    "from keras.utils import to_categorical  # For converting labels to one-hot encoding\n",
    "from keras.models import Model # For defining the model\n",
    "from keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, LSTM, GRU, Dropout, Reshape, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D, Bidirectional, BatchNormalization # For defining model layers\n",
    "from keras.callbacks import EarlyStopping # For early stopping during training\n",
    "from keras.optimizers import Adam # For optimization\n",
    "from keras.regularizers import l2 \n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt # For plotting\n",
    "\n",
    "# Labels for the activities\n",
    "labels = ['jogging', 'stretching (triceps)', 'jogging (rotating arms)',\n",
    "          'stretching (lunging)', 'jogging (skipping)', 'push-ups',\n",
    "          'push-ups (complex)', 'jogging (sidesteps)',\n",
    "          'stretching (shoulders)', 'sit-ups', 'stretching (hamstrings)',\n",
    "          'sit-ups (complex)', 'lunges', 'burpees',\n",
    "          'stretching (lumbar rotation)', 'jogging (butt-kicks)',\n",
    "          'bench-dips', 'lunges (complex)']\n",
    "\n",
    "def load_and_preprocess_single_csv(csv_file, scaler=None, label_encoder=None, fit_scaler=False, fit_label_encoder=False):\n",
    "    # Load the data from the CSV file\n",
    "    data = pd.read_csv(csv_file)\n",
    "    \n",
    "    # Clean the data: drop rows with NaN values in 'label' and interpolate remaining NaNs\n",
    "    data = data.dropna(subset=['label'])\n",
    "    data = data.interpolate(method='linear', limit_direction='forward', axis=0).dropna()\n",
    "    \n",
    "    # Separate features (X) and labels (y)\n",
    "    X = data.drop(['sbj_id', 'label'], axis=1).values\n",
    "    y = data['label'].values\n",
    "    \n",
    "    # Fit or transform the features using the scaler\n",
    "    if fit_scaler:\n",
    "        X = scaler.fit_transform(X)\n",
    "    else:\n",
    "        X = scaler.transform(X)\n",
    "    \n",
    "    # Fit or transform the labels using the label encoder and convert to one-hot encoding\n",
    "    if fit_label_encoder:\n",
    "        y = to_categorical(label_encoder.fit_transform(y))\n",
    "    else:\n",
    "        y = to_categorical(label_encoder.transform(y))\n",
    "    \n",
    "    return X, y\n",
    "\n",
    "def build_model(hp, input_dim, num_classes):\n",
    "    # Define the input shape\n",
    "    input_shape = (10, input_dim)  # Assume at least 10 time steps for Conv1D to work properly\n",
    "    \n",
    "    input_layer = Input(shape=input_shape)\n",
    "    # Hyperparameters for tuning\n",
    "    conv_filters = hp.Int('conv_filters', min_value=32, max_value=128, step=32)\n",
    "    kernel_size = hp.Int('kernel_size', min_value=1, max_value=3, step=1)\n",
    "    lstm_units = hp.Int('lstm_units', min_value=64, max_value=256, step=64)\n",
    "    gru_units = hp.Int('gru_units', min_value=64, max_value=256, step=64)\n",
    "    dropout_rate = hp.Float('dropout_rate', min_value=0.2, max_value=0.5, step=0.1)\n",
    "    l2_reg = hp.Float('l2_reg', min_value=0.01, max_value=0.1, step=0.01)\n",
    "\n",
    "    # Convolutional layer\n",
    "    cnn_out = Conv1D(filters=conv_filters, kernel_size=kernel_size, activation='relu', kernel_regularizer=l2(l2_reg))(input_layer)\n",
    "    cnn_out = BatchNormalization()(cnn_out)\n",
    "    cnn_out = MaxPooling1D(pool_size=2)(cnn_out)  # Use pooling size 2\n",
    "    cnn_out = Flatten()(cnn_out)\n",
    "    cnn_out = Dense(100, activation='relu', kernel_regularizer=l2(l2_reg))(cnn_out)\n",
    "    cnn_out = Dropout(dropout_rate)(cnn_out)\n",
    "\n",
    "    # Reshape for LSTM input\n",
    "    reshaped_cnn_out = Reshape((1, 100))(cnn_out)\n",
    "\n",
    "    # Bidirectional LSTM layer\n",
    "    lstm_out = Bidirectional(LSTM(lstm_units, activation='relu', return_sequences=True))(reshaped_cnn_out)\n",
    "    lstm_out = Dropout(dropout_rate)(lstm_out)\n",
    "\n",
    "    # Bidirectional GRU layer\n",
    "    gru_out = Bidirectional(GRU(gru_units, activation='relu', return_sequences=True))(lstm_out)\n",
    "    gru_out = Dropout(dropout_rate)(gru_out)\n",
    "\n",
    "    # Transformer encoder layer\n",
    "    def transformer_encoder(inputs, num_heads, key_dim, ff_dim, dropout=0.1):\n",
    "        attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)(inputs, inputs)\n",
    "        attention_output = Dropout(dropout)(attention_output)\n",
    "        attention_output = LayerNormalization(epsilon=1e-6)(attention_output + inputs)\n",
    "\n",
    "        ff_output = Dense(ff_dim, activation='relu')(attention_output)\n",
    "        ff_output = Dropout(dropout)(ff_output)\n",
    "        ff_output = Dense(inputs.shape[-1])(ff_output)\n",
    "        encoder_output = LayerNormalization(epsilon=1e-6)(ff_output + attention_output)\n",
    "        \n",
    "        return encoder_output\n",
    "\n",
    "    # Apply transformer encoder\n",
    "    transformer_out = transformer_encoder(gru_out, num_heads=4, key_dim=128, ff_dim=256, dropout=dropout_rate)\n",
    "    transformer_out = GlobalAveragePooling1D()(transformer_out)\n",
    "\n",
    "    # Output layer\n",
    "    output_layer = Dense(num_classes, activation='softmax')(transformer_out)\n",
    "\n",
    "    # Compile the model\n",
    "    model = Model(inputs=input_layer, outputs=output_layer)\n",
    "    learning_rate = hp.Float('learning_rate', min_value=1e-4, max_value=1e-2, sampling='LOG')\n",
    "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(csv_files, n_splits=6, max_trials=10, executions_per_trial=1):\n",
    "    random.shuffle(csv_files)\n",
    "    all_X = []\n",
    "    all_y = []\n",
    "\n",
    "    # Load all data for cross-validation\n",
    "    scaler = StandardScaler()\n",
    "    label_encoder = LabelEncoder()\n",
    "    for csv_file in csv_files:\n",
    "        X, y = load_and_preprocess_single_csv(csv_file, scaler, label_encoder, fit_scaler=True, fit_label_encoder=True)\n",
    "        all_X.append(X)\n",
    "        all_y.append(y)\n",
    "    \n",
    "    all_X = np.concatenate(all_X, axis=0)\n",
    "    all_y = np.concatenate(all_y, axis=0)\n",
    "\n",
    "    # Ensure we have enough time steps\n",
    "    time_steps = 10\n",
    "    if all_X.shape[0] % time_steps != 0:\n",
    "        pad_size = time_steps - (all_X.shape[0] % time_steps)\n",
    "        all_X = np.pad(all_X, ((0, pad_size), (0, 0)), mode='constant')\n",
    "        all_y = np.pad(all_y, ((0, pad_size), (0, 0)), mode='constant')\n",
    "\n",
    "    all_X = all_X.reshape(-1, time_steps, all_X.shape[1])\n",
    "    all_y = all_y.reshape(-1, time_steps, all_y.shape[1])\n",
    "    all_y = all_y[:, 0, :]  # Use the first label of each sequence\n",
    "\n",
    "    # Define the tuner\n",
    "    tuner = kt.RandomSearch(\n",
    "        hypermodel=lambda hp: build_model(hp, input_dim=all_X.shape[2], num_classes=all_y.shape[1]),\n",
    "        objective='val_accuracy',\n",
    "        max_trials=max_trials,\n",
    "        executions_per_trial=executions_per_trial,\n",
    "        directory='hyperparam_tuning',\n",
    "        project_name='wearable_data'\n",
    "    )\n",
    "\n",
    "    # Cross-validation\n",
    "    kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    test_accuracies = []\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    for train_index, test_index in kf.split(all_X, np.argmax(all_y, axis=1)):\n",
    "        X_train, X_test = all_X[train_index], all_X[test_index]\n",
    "        y_train, y_test = all_y[train_index], all_y[test_index]\n",
    "\n",
    "        # Compute class weights to handle class imbalance\n",
    "        class_weights = compute_class_weight('balanced', classes=np.unique(np.argmax(y_train, axis=1)), y=np.argmax(y_train, axis=1))\n",
    "        class_weights = {i: weight for i, weight in enumerate(class_weights)}\n",
    "\n",
    "        # Early stopping callback\n",
    "        stop_early = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "        # Hyperparameter tuning\n",
    "        tuner.search(X_train, y_train, epochs=150, batch_size=64, validation_split=0.2, callbacks=[stop_early], class_weight=class_weights)\n",
    "\n",
    "        # Get the best model\n",
    "        best_model = tuner.get_best_models(num_models=1)[0]\n",
    "\n",
    "        # Evaluate the best model\n",
    "        _, test_accuracy = best_model.evaluate(X_test, y_test, verbose=0)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Make predictions\n",
    "        y_pred = np.argmax(best_model.predict(X_test), axis=1)\n",
    "        all_predictions.append(y_pred)\n",
    "        all_true_labels.append(np.argmax(y_test, axis=1))\n",
    "\n",
    "    # Compute overall accuracy\n",
    "    overall_accuracy = np.mean(test_accuracies)\n",
    "\n",
    "    # Flatten predictions for classification report\n",
    "    all_predictions = np.concatenate(all_predictions)\n",
    "    all_true_labels = np.concatenate(all_true_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    average_f1 = f1_score(all_true_labels, all_predictions, average='weighted')\n",
    "    average_recall = recall_score(all_true_labels, all_predictions, average='weighted')\n",
    "    average_precision = precision_score(all_true_labels, all_predictions, average='weighted')\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    conf_matrix = confusion_matrix(all_true_labels, all_predictions)\n",
    "\n",
    "    print(\"\\nOverall Test Accuracy:\", overall_accuracy)\n",
    "    for i, test_accuracy in enumerate(test_accuracies):\n",
    "        print(f\"Fold {i + 1} Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Overall Test Accuracy: {overall_accuracy:.4f}\")\n",
    "    print(f\"Average Precision Score: {average_precision:.4f}\")\n",
    "    print(f\"Average Recall Score: {average_recall:.4f}\")\n",
    "    print(f\"Average F1 Score: {average_f1:.4f}\")\n",
    "\n",
    "    # Plot label matrix with annotations\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(conf_matrix, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "\n",
    "    # Add numerical annotations\n",
    "    for i in range(conf_matrix.shape[0]):\n",
    "        for j in range(conf_matrix.shape[1]):\n",
    "            plt.text(j, i, format(conf_matrix[i, j], 'd'),\n",
    "                     horizontalalignment=\"center\",\n",
    "                     color=\"white\" if conf_matrix[i, j] > np.max(conf_matrix) / 2. else \"black\")\n",
    "    # Plotting the confusion matrix\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(labels))\n",
    "    plt.xticks(tick_marks, labels, rotation=45, ha='right') \n",
    "    plt.yticks(tick_marks, labels)\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "\n",
    "# Loading the files\n",
    "if __name__ == \"__main__\":\n",
    "    # List of all CSV files\n",
    "    csv_files = [\n",
    "        'data/wear/raw/inertial/sbj_0.csv',\n",
    "        'data/wear/raw/inertial/sbj_1.csv',\n",
    "        'data/wear/raw/inertial/sbj_2.csv',\n",
    "        'data/wear/raw/inertial/sbj_3.csv',\n",
    "        'data/wear/raw/inertial/sbj_4.csv',\n",
    "        'data/wear/raw/inertial/sbj_5.csv',\n",
    "        'data/wear/raw/inertial/sbj_6.csv',\n",
    "        'data/wear/raw/inertial/sbj_7.csv',\n",
    "        'data/wear/raw/inertial/sbj_8.csv',\n",
    "        'data/wear/raw/inertial/sbj_9.csv',\n",
    "        'data/wear/raw/inertial/sbj_10.csv',\n",
    "        'data/wear/raw/inertial/sbj_11.csv',\n",
    "        'data/wear/raw/inertial/sbj_12.csv',\n",
    "        'data/wear/raw/inertial/sbj_13.csv',\n",
    "        'data/wear/raw/inertial/sbj_14.csv',\n",
    "        'data/wear/raw/inertial/sbj_15.csv',\n",
    "        'data/wear/raw/inertial/sbj_16.csv',\n",
    "        'data/wear/raw/inertial/sbj_17.csv',\n",
    "    ]\n",
    "\n",
    "    # Perform training and testing with different hyperparameters\n",
    "    train_and_evaluate(csv_files, n_splits=6, max_trials=10, executions_per_trial=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aade244-8a34-45ca-8c28-1ad20b9466d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
